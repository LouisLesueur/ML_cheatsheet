<!doctype html>
<html>
   <head>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
      <meta name="generator" content="pandoc">
      <meta name="author" content="Louis Lesueur">
      
      <title>Machine learning - synthesis</title>
      <!-- Bootstrap -->
      <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">
      <!-- Font-awesome -->
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
      <!-- Styles -->
      <link rel="stylesheet" href="https://ashki23.github.io/styles.css">
            <!-- Add favicon here -->
            <!-- Add site-verifications here -->
            <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js"></script>
              <script>document.addEventListener("DOMContentLoaded", function () {
               var mathElements = document.getElementsByClassName("math");
               var macros = [];
               for (var i = 0; i < mathElements.length; i++) {
                var texText = mathElements[i].firstChild;
                if (mathElements[i].tagName == "SPAN") {
                 katex.render(texText.data, mathElements[i], {
                  displayMode: mathElements[i].classList.contains('display'),
                  throwOnError: false,
                  macros: macros,
                  fleqn: false
                 });
              }}});
              </script>
              <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" />
         </head>
   <body>
            <!-- From https://getbootstrap.com/docs/4.5/components/navbar/ -->
            <nav class="navbar fixed-top navbar-expand-lg navbar-dark bg-dark">
               <a class="navbar-brand" href="#">Machine Learning synthesis</a>

            </nav>
            <div class="container">
         <h1 class="title">Machine learning - synthesis</h1>
                  <div class="row">
            <div class="col-xl-10"><h2 id="data-representation">Data representation</h2>
<h3 id="definitions">Definitions</h3>
<ul>
<li><span class="math inline">X</span>: real random vector taking values in <span class="math inline">\mathcal{X}</span></li>
<li><span class="math inline">Y</span>: real random vector taking values in <span class="math inline">\mathcal{Y}</span>
<ul>
<li>For a regression problem: <span class="math inline">\mathcal{Y}=\mathbb{R}</span></li>
<li>For a classification problem: <span class="math inline">\mathcal{Y} = \{C_k \}_{k&lt;K}</span> (<span class="math inline">K</span> classes)</li>
</ul></li>
<li><span class="math inline">p_{XY}</span> joint distribution of <span class="math inline">(X,Y)</span>, unknown</li>
</ul>
<h3 id="pre-treatments">Pre-treatments</h3>
<p>For a dataset <span class="math inline">\mathcal{X}_n</span>, composed of <span class="math inline">n</span> samples from <span class="math inline">X</span> we note:</p>
<ul>
<li><span class="math inline">x</span> the dataset</li>
<li><span class="math inline">x_i</span> the <span class="math inline">i</span>-th sample</li>
<li><span class="math inline">x_i^{(j)}</span> the <span class="math inline">j</span>-th feature of the <span class="math inline">i</span>-th sample</li>
</ul>
<p>Several pre-treatments can be applied:</p>
<h4 id="one-hot-encoding">One-Hot encoding</h4>
<p>To encode string features with a finite amount of values possibles <span class="math inline">l</span>, it is better to encode them in a <span class="math inline">l</span> long binary vector. For example, if the feature is “color”, and the possible values are “red”, “yellow” and “green”, they’ll be eoncoded as:</p>
<ul>
<li>red = [1,0,0]</li>
<li>yellow = [0,1,0]</li>
<li>green = [0,0,1]</li>
</ul>
<p>This ensures that the euclidian distance between each color is the same and won’t damage the learning.</p>
<h4 id="feature-rescaling-normalization-and-standardization">Feature rescaling (normalization and standardization)</h4>
<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="header">
<th>Normalization</th>
<th>Standardization</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">x^{(j)} \leftarrow \frac{x^{(j)} - min^{(j)}}{max^{(j)} - min^{(j)}}</span></td>
<td><span class="math inline">x^{(j)} \leftarrow \frac{x^{(j)} - \mu^{(j)}}{\sigma^{(j)}}</span></td>
</tr>
</tbody>
</table>
<p>To chose between the two there is no general rule, but:</p>
<ul>
<li>In practice, unsupervised algorithms more often benefit from standardization than from normalization</li>
<li>Standardization is better if the feature is already distributed along a gaussian</li>
<li>Standardization is also better if the dataset contains outliers (because normalization squeeze them)</li>
<li>In all other cases, normalization has to be prefered because it ensures that all features are in the same range, and that not anyone will dominate during optimization. It also prevents overflows in calculs</li>
</ul>
<h3 id="handling-missing-data">Handling missing data</h3>
<p>When some feature values are missing from the dataset, one can: + Remove the corresponding data sample (if the dataset is big enough) + Use a learning algorithm that can deal with missing features + Use a data imputation technique</p>
<h4 id="data-imputation-techniques">Data imputation techniques</h4>
<ul>
<li>Replace the missing value by an average value of the feature in the dataset</li>
<li>Replace it by a value outside the normal range, the idea is that the algorithm will learn what to do when a feature has a value different from others</li>
<li>Replace it by a median value in the normal range, the idea is that a median value won’t have a big impact on the prediction.</li>
<li>Use all the remaining features to built the missing using a regression algorithm</li>
</ul>
<h3 id="dimensionality-reduction">Dimensionality reduction</h3>
<p>Finally, if the dataset dimension is too big, the optimization will be slow because of the curse of dimensionnality. One way to accelerate the training is to reduce dimensionnality, by keeping the maximum of information using PCA or other algorithms.</p>
<h2 id="discriminative-problems">Discriminative problems</h2>
<p>In discriminative problems, we have a labelized sample <span class="math inline">\mathcal{D}_n = \{(x_i, y_i) \in \mathcal{X}\times\mathcal{Y}\}</span>, from <span class="math inline">p_{XY}</span>, and we want to directly determine <span class="math inline">p(y | x)</span></p>
<h3 id="loss-and-risk-functions">Loss and risk functions</h3>
<p>Goal of a machine learning algorthm: find function <span class="math inline">f</span> that can predict new values with the best accuracy.</p>
<ul>
<li>Loss function <span class="math inline">L(y_{pred},y)</span>: measure the error of the predictor.</li>
<li>Risk function <span class="math inline">R(f) = \mathbb{E}_{XY}(L(Y,f(X)))</span></li>
<li>Empirical risk function <span class="math inline">R_{emp}(f) = \frac{1}{n} \sum_i L(y_i, f(x_i))</span></li>
</ul>
<p>The problem real solution is <span class="math display">
f^* = \arg \min_f R(f)
</span></p>
<p>To solve the problem, we chose a restricted set of possible function <span class="math inline">\mathcal{F}</span> (either parametric model, either not). We note: <span class="math display">
\tilde{f} = \arg \min_{f \in \mathcal{F}} R(f)
</span></p>
<p>As we only have a sample <span class="math inline">\mathcal{D_n}</span> of the distribution <span class="math inline">p_{XY}</span>, which is unknown, in practise we compute: <span class="math display">
\hat{f} = \arg \min_{f \in \mathcal{F}} R_{emp}(f)
</span></p>
<p>Errors:</p>
<ul>
<li>approximation: <span class="math inline">f^*-\tilde{f}</span></li>
<li>estimation: <span class="math inline">\tilde{f}-\hat{f}</span></li>
<li>total: <span class="math inline">f^* - \hat{f}</span></li>
</ul>
<h3 id="parametric-models">Parametric models</h3>
<h4 id="maximum-likelihood-principle">Maximum Likelihood principle</h4>
<p>In Machine Learning, most of the time we make assumption on data:</p>
<p><span class="math display">
y = f_\theta(x) + \epsilon
</span></p>
<p>with <span class="math inline">\epsilon</span> iid zero mean noise.</p>
<p>So : <span class="math inline">y \sim F(f_\theta(x))</span></p>
<p>Let <span class="math inline">p_\theta</span> be the estimated parametric distribution of <span class="math inline">y|x</span>, then its log-likelihood is defined by:</p>
<p><span class="math display">
\mathcal{L}(f_\theta) = \sum_{i=0}^n \ln{p_\theta(y_i|x_i)}
</span></p>
<p>The corresponding MLE estimator is: <span class="math inline">\hat{f_\theta} = \arg \max_{\theta} \mathcal{L(f_\theta)}</span></p>
<p>MLE can be seen as a particular case of risk minimization, with <span class="math inline">L(f_\theta(x), y)=-\ln{p_\theta(y|x)}</span> (for a <span class="math inline">L^2</span> loss, with a linear model and a normal error, the Least-square model naturally appears). And we have:</p>
<p><span class="math display">
R_{emp}(f_\theta) = - \frac{1}{n} \sum_{i=0}^n \ln{p_\theta(y_i|x_i)}
</span></p>
<p><span class="math display">
R(f_\theta) = \mathbb{E}(-\ln{p_\theta(Y|X)})
</span></p>
<p><span class="math display">
R(f_\theta) - R(f^*) = \text{KL}(p_\theta, p^*)
</span></p>
<h4 id="loss-minimization">Loss minimization</h4>
<p>So, minimizing the risk (or maximizing the likelihood) often leads to a problem of the form:</p>
<p><span class="math display">
\min_\theta \frac{1}{n} \sum_{i=1}^{n} L( f_\theta(x_i), y_i ) + \lambda C(\theta)
</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">L</span> is a loss function</li>
<li><span class="math inline">f_\theta</span> is the prediction function (<span class="math inline">\theta</span> is the parameter to optimize)</li>
<li><span class="math inline">C</span> is a regularization function and <span class="math inline">\lambda</span> a regularization factor</li>
</ul>
<h5 id="linear-case">Linear case</h5>
<p>Here the model are linear, <span class="math inline">f_\theta(x) = \theta^Tx</span>. For this kind of mdel, Fenchel duality can be used to simply express the dual problem, which can be exploited for the kernel trick.</p>
<table>
<colgroup>
<col style="width: 37%" />
<col style="width: 62%" />
</colgroup>
<thead>
<tr class="header">
<th>Primal</th>
<th>Dual</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\min_\theta \frac{1}{n} \sum_{i=1}^{n} L(y_i, \theta^Tx_i) + C(\theta)</span></td>
<td><span class="math inline">\max_\alpha \frac{1}{n} \sum_{i=1}^{n} -L^{\#}(y_i,-\alpha_i ) - C^{\#}(\sum_{i=1}^{n} \alpha_i x_i)</span></td>
</tr>
</tbody>
</table>
<p>Where <span class="math inline">L^\#</span> and <span class="math inline">C^\#</span> are the convex conjugate of <span class="math inline">L</span> (for the second variable) and <span class="math inline">C</span> (defined by <span class="math inline">f^\#(a) = \max_z (za - f(z))</span>). And <span class="math inline">x_i^\#(\cdot)</span> is the adjoint operator of <span class="math inline">(\cdot^Tx_i)</span> (<span class="math inline">{\theta^T}^\# = \overline{\theta}</span>)</p>
<p>It can be shown that if <span class="math inline">\hat{\theta}</span> and <span class="math inline">\hat{\alpha}</span> are the optimal solutions of those problems, then we have: <span class="math display">
P(\hat{\theta}) = P(\theta(\hat{\alpha})) = D(\hat{\alpha})
</span></p>
<h5 id="the-kernel-trick-for-non-linear-models">The kernel trick for non-linear models</h5>
<p>When there are only scalar products between data in the problem (which is the case for the dual in general), one can use the kernel trick. For a mapping <span class="math inline">\phi</span> of the data in another space, the scalar product can be computed using a well chosen kernel function (symmetric and positive semi-definite, Mercer theorem)</p>
<p><span class="math display">
k(x,y) = \phi(x) \cdot \phi(y)
</span></p>
<p>Common kernels:</p>
<table>
<thead>
<tr class="header">
<th>Name</th>
<th><span class="math inline">k(x,y)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Linear</td>
<td><span class="math inline">x^Ty</span></td>
</tr>
<tr class="even">
<td>Polynomial</td>
<td><span class="math inline">(r+x^Ty)^p</span></td>
</tr>
<tr class="odd">
<td>Gaussian</td>
<td><span class="math inline">\exp(-\frac{||x-y||^2}{2 \sigma ^2})</span></td>
</tr>
<tr class="even">
<td>Exponential</td>
<td><span class="math inline">\exp(-\alpha ||x-y||)</span></td>
</tr>
<tr class="odd">
<td>tanh</td>
<td><span class="math inline">\tanh(\alpha x^Ty + c)</span></td>
</tr>
</tbody>
</table>
<h5 id="algorithms-to-solve-the-minimization-problem">Algorithms to solve the minimization problem</h5>
<p>We note:</p>
<ul>
<li><span class="math inline">J_i(\theta) = L( f_\theta(x_i), y_i ) + n\lambda C(\theta)</span></li>
<li><span class="math inline">J(\theta) = \frac{1}{n} \sum_i J_i(\theta)</span></li>
</ul>
<p>Descent methods (<span class="math inline">i_k</span> is a random index):</p>
<table>
<colgroup>
<col style="width: 37%" />
<col style="width: 25%" />
<col style="width: 37%" />
</colgroup>
<thead>
<tr class="header">
<th>Name</th>
<th>type</th>
<th>update rule</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>FG</td>
<td>primal</td>
<td><span class="math inline">\theta \leftarrow \theta - \gamma \nabla_\theta J(\theta)</span></td>
</tr>
<tr class="even">
<td>SGD</td>
<td>primal</td>
<td><span class="math inline">\theta \leftarrow \theta - \gamma \nabla_\theta J_{i_k}(\theta)</span></td>
</tr>
<tr class="odd">
<td>Newton</td>
<td>primal</td>
<td><span class="math inline">\theta \leftarrow \theta - Hess^{-1}_\theta(J)\nabla_\theta J(\theta)</span></td>
</tr>
<tr class="even">
<td>SAG</td>
<td>primal</td>
<td><span class="math inline">\theta \leftarrow \theta - \frac{\gamma}{n} \sum_i(\nabla_\theta J_{i_k}(\theta) \mathbb{1}_{i=i_k}+y_i\mathbb{1}_{i\neq i_k})</span></td>
</tr>
</tbody>
</table>
<p>Other algorithms:</p>
<table>
<thead>
<tr class="header">
<th>Name</th>
<th>type</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>BFGS</td>
<td>primal</td>
</tr>
<tr class="even">
<td>SMO</td>
<td>dual</td>
</tr>
<tr class="odd">
<td>SDCA</td>
<td>primal-dual</td>
</tr>
</tbody>
</table>
<p>To solve a linear system: conjugate gradient</p>
<h4 id="loss-functions-and-corresponding-problems">Loss functions and corresponding problems:</h4>
<p>A loss function is defined by:</p>
<p><span class="math display">
L: \mathcal{Y} \times \mathcal{Y} \rightarrow [0, +\infty )
</span></p>
<p>Detailed loss function theory in <em>How to compare different loss functions and their risks.</em></p>
<h5 id="glm-generalized-linear-models">GLM (Generalized Linear Models)</h5>
<p>General linear models are defined by the following assumptions on data:</p>
<ul>
<li>Exponential family (cf table): <span class="math inline">p_\theta(y|x) = b(y) \exp{(\chi T(y) - a(\eta))}</span></li>
<li><span class="math inline">f_\theta(x) = \mathbb{E}_\theta(y|x)</span></li>
<li><span class="math inline">\chi = \theta^Tx</span></li>
</ul>
<p>By writing the corresponding loglikelihood, these GLM leads to the most famous problems of Machine Learning (Least square regression, logistic regression, and softmax regression in particular, cf next subsections)</p>
<table>
<colgroup>
<col style="width: 15%" />
<col style="width: 20%" />
<col style="width: 22%" />
<col style="width: 32%" />
<col style="width: 9%" />
</colgroup>
<thead>
<tr class="header">
<th>Distribution</th>
<th><span class="math inline">\chi</span></th>
<th><span class="math inline">T(y)</span></th>
<th><span class="math inline">a(\chi)</span></th>
<th><span class="math inline">b(y)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Bernoulli</td>
<td><span class="math inline">\ln{\frac{\mu}{1-\mu}}</span></td>
<td>y</td>
<td><span class="math inline">\ln(1+e^{\chi})</span></td>
<td>1</td>
</tr>
<tr class="even">
<td>Gaussian</td>
<td><span class="math inline">\mu</span></td>
<td>y</td>
<td><span class="math inline">\frac{\chi^2}{2}</span></td>
<td><span class="math inline">\frac{1}{\sqrt{2\pi}} \exp(\frac{-y^2}{2})</span></td>
</tr>
<tr class="odd">
<td>Poisson</td>
<td><span class="math inline">\ln{\mu}</span></td>
<td>y</td>
<td><span class="math inline">\exp{\chi}</span></td>
<td><span class="math inline">\frac{1}{y!}</span></td>
</tr>
<tr class="even">
<td>Geometric</td>
<td><span class="math inline">\ln{(1-\mu)}</span></td>
<td>y</td>
<td><span class="math inline">\ln \frac{e^\chi}{1-e^\chi}</span></td>
<td>1</td>
</tr>
<tr class="odd">
<td>Multinomial (<span class="math inline">k</span> classes)</td>
<td><span class="math inline">[\ln\frac{\mu_i}{\mu_k}]_{i&lt;k}</span></td>
<td><span class="math inline">T(i) = (0...1...0)</span> (defined on integers between 0 and k-1, 1 in the i-th position)</td>
<td><span class="math inline">-\ln(\mu_k)</span></td>
<td>1</td>
</tr>
</tbody>
</table>
<h5 id="binary-classification">Binary classification</h5>
<p>Binary classifier: <span class="math inline">g: \mathcal{X} \rightarrow \{-1,1\}</span></p>
<p>In general, <span class="math inline">g</span> is decomposed as: <span class="math inline">g(x) = sgn(f(x))</span>, with:</p>
<ul>
<li><span class="math inline">f(x) = h(\eta(x))</span>, the predictor
<ul>
<li><span class="math inline">h: [0,1] \rightarrow \mathbb{R}</span> a link function</li>
<li><span class="math inline">\eta(x) = \mathbb{P}_{Y | X}(1 | x)</span>, the learned probability</li>
</ul></li>
</ul>
<p>The optimal link function <span class="math inline">h^*</span> can generaly be obtained analiticaly, so classification problems are reduced to find an estimation <span class="math inline">\hat{\eta}</span> of <span class="math inline">\eta</span>.</p>
<p><strong>To do so, one can set: <span class="math inline">f(x) = f_\theta(x)</span> (<span class="math inline">=\theta^Tx</span> most of the time), minimize the empirical risk (using the corresponding loss), and deduce an optimal predictor <span class="math inline">\hat{h}</span>, and <span class="math inline">\hat{\eta}(x) = h^{*-1}(\hat{h}(x))</span></strong></p>
<p>For classification problem, loss can be written: <span class="math inline">L(y,x) = \phi(yh(x))</span> (margin-based loss functions)</p>
<p>The corresponding conditionnal risk: <span class="math inline">C_\phi(\eta, h) = \eta \phi(h) + (1-\eta) \phi(-h)</span></p>
<table>
<colgroup>
<col style="width: 8%" />
<col style="width: 14%" />
<col style="width: 11%" />
<col style="width: 11%" />
<col style="width: 22%" />
<col style="width: 12%" />
<col style="width: 18%" />
</colgroup>
<thead>
<tr class="header">
<th>Name</th>
<th><span class="math inline">\phi(v)</span></th>
<th><span class="math inline">h^*_\phi(\eta)</span></th>
<th><span class="math inline">h^{*-1}_\phi(v)</span></th>
<th><span class="math inline">C^*_\phi(\eta)</span></th>
<th><span class="math inline">L^{\#}(y,-a)</span></th>
<th>properties</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">0/1</span></td>
<td><span class="math inline">sgn(v)</span></td>
<td><span class="math inline">sgn(2\eta -1)</span></td>
<td></td>
<td></td>
<td></td>
<td>Not used in practise because <span class="math inline">NP</span> hard</td>
</tr>
<tr class="even">
<td>square</td>
<td><span class="math inline">(1-v)^2</span></td>
<td><span class="math inline">2\eta -1</span></td>
<td><span class="math inline">\frac{1}{2}(v+1)</span></td>
<td><span class="math inline">4\eta(1-\eta)</span></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>modified LS</td>
<td><span class="math inline">\max(1-v,0)^2</span></td>
<td><span class="math inline">2\eta -1</span></td>
<td>NA</td>
<td><span class="math inline">4\eta(1-\eta)</span></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>SVM</td>
<td><span class="math inline">\max(1-v,0)</span></td>
<td><span class="math inline">sgn(2\eta-1)</span></td>
<td>NA</td>
<td><span class="math inline">1-|2\eta -1 |</span></td>
<td>-ay</td>
<td></td>
</tr>
<tr class="odd">
<td>Boosting</td>
<td><span class="math inline">e^{-v}</span></td>
<td><span class="math inline">\frac{1}{2} \ln \frac{\eta}{1-\eta}</span></td>
<td><span class="math inline">\frac{e^{2v}}{1+e^{2v}}</span></td>
<td><span class="math inline">2 \sqrt{\eta(1-\eta)}</span></td>
<td></td>
<td>The loss of a mis-prediction increases exponentially with the value of <span class="math inline">-v</span></td>
</tr>
<tr class="even">
<td>Logistic</td>
<td><span class="math inline">\ln(1+e^{-v})</span></td>
<td><span class="math inline">\ln \frac{\eta}{1-\eta}</span></td>
<td><span class="math inline">\frac{e^v}{1+e^v}</span></td>
<td><span class="math inline">-\eta \ln{\eta} - (1-\eta) \ln{(1-\eta)}</span></td>
<td><span class="math inline">ay \ln{ay} + (1-ay) \ln(1-ay)</span></td>
<td>It is a GLM (for Bernoulli distribution), equivalent to cross-entropy loss</td>
</tr>
<tr class="odd">
<td>Savage</td>
<td><span class="math inline">\frac{1}{(1+e^v)^2}</span></td>
<td><span class="math inline">\ln \frac{\eta}{1-\eta}</span></td>
<td><span class="math inline">\frac{e^v}{1+e^v}</span></td>
<td><span class="math inline">\eta(1-\eta)</span></td>
<td></td>
<td> non-convex, better for outliers</td>
</tr>
<tr class="even">
<td>Tangent</td>
<td><span class="math inline">(2 \arctan(v)-1)^2</span></td>
<td><span class="math inline">\tan(\eta - \frac{1}{2})</span></td>
<td><span class="math inline">\arctan(v) + \frac{1}{2}</span></td>
<td><span class="math inline">4\eta(1-\eta)</span></td>
<td></td>
<td> non-convex, better for outliers</td>
</tr>
</tbody>
</table>
<figure>
<img src="plots/5206064366391498750.png" />
</figure>
<h5 id="multilabel-classification">Multilabel classification</h5>
<h6 id="softmax">Softmax</h6>
<p>It is built from multinomial GLM. Inversing its link function gives:</p>
<p><span class="math display">
\phi_i = \frac{e^{\eta_i}}{\sum_{j=1}^k e^{\eta_i}}
</span></p>
<p>The corresponding predictor:</p>
<p><span class="math display">
h_\theta(x) = (\frac{e^{\theta_i^T x}}{\sum_{j=1}^k e^{\theta_j^T x}})
</span></p>
<p>And the log Likelihood to maximize:</p>
<p><span class="math display">
l(\theta) = \sum_{i=0}^n \ln( \prod_{l=1}^k \frac{e^{\theta_l^T x_i}}{\sum_{j=1}^k e^{\theta_j^T x_i}})^{\mathbb{1}_{y_i = l}}
</span></p>
<p>The minimization problem is therefor:</p>
<p><span class="math display">
\min_\theta - \sum_{i=0}^n y_i \ln(\frac{e^{\theta_l^T x_i}}{\sum_{j=1}^k e^{\theta_j^T x_i}})
</span></p>
<h5 id="regression">Regression</h5>
<table>
<colgroup>
<col style="width: 12%" />
<col style="width: 35%" />
<col style="width: 25%" />
<col style="width: 27%" />
</colgroup>
<thead>
<tr class="header">
<th>Name</th>
<th><span class="math inline">L(y,x)</span></th>
<th><span class="math inline">L^(y,-a)</span></th>
<th>properties</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>square</td>
<td><span class="math inline">(y-x)^2</span></td>
<td><span class="math inline">-ay + \frac{a^2}{4}</span></td>
<td>estimates mean label, sensitive to outliers, differentiable everywhere</td>
</tr>
<tr class="even">
<td>absolute</td>
<td><span class="math inline">|y-x|</span></td>
<td>-ay</td>
<td>estimates median label, less sensitive to noise, not differentiable in 0</td>
</tr>
<tr class="odd">
<td>Huber</td>
<td> <span class="math inline">\frac{1}{2} (y-x)^2</span> if <span class="math inline">|x-y| &lt; \delta</span>, <span class="math inline">\delta(|y-x| - \frac{\delta}{2})</span> otherwise</td>
<td></td>
<td>“Best of Both Worlds” of Squared and Absolute Loss , Takes on behavior of Squared-Loss when loss is small, and Absolute Loss when loss is large. Once differentiable</td>
</tr>
<tr class="even">
<td>log-ch</td>
<td><span class="math inline">\ln(\cosh(y-x))</span></td>
<td></td>
<td>Similar to Huber Loss, but twice differentiable everywhere</td>
</tr>
</tbody>
</table>
<figure>
<img src="plots/13146123143602842735.png" />
</figure>
<h5 id="regularizations">Regularizations</h5>
<table>
<colgroup>
<col style="width: 27%" />
<col style="width: 21%" />
<col style="width: 51%" />
</colgroup>
<thead>
<tr class="header">
<th>Name</th>
<th><span class="math inline">C(w)</span></th>
<th>properties</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">L^2</span></td>
<td><span class="math inline">||w||^2_2</span></td>
<td>strictly convex (1 solution), differentiable, but relies on all features (dense solution)</td>
</tr>
<tr class="even">
<td><span class="math inline">L^1</span></td>
<td><span class="math inline">||w||_1</span></td>
<td>convex, not differentiable, but performs feature selection (sparse solution)</td>
</tr>
<tr class="odd">
<td>Elastic net</td>
<td><span class="math inline">\alpha ||w||_1 + (1-\alpha)||w||^2_2</span></td>
<td>strictly convex, not differentialbe</td>
</tr>
<tr class="even">
<td><span class="math inline">L^p</span> (often <span class="math inline">0&lt;p&lt;1</span>)</td>
<td><span class="math inline">||w||_p</span></td>
<td>non convex, very sparse solutions, initialization dependant, not differentiable</td>
</tr>
</tbody>
</table>
<h5 id="focus-on-linear-regression">Focus on linear regression</h5>
<table>
<thead>
<tr class="header">
<th>Name</th>
<th>loss</th>
<th>regularizer</th>
<th>Solution</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>OLS (ordinary least square)</td>
<td>square</td>
<td> NA</td>
<td><span class="math inline">\theta = (xx^T)^{-1}xy^T</span></td>
</tr>
<tr class="even">
<td>Ridge regression</td>
<td>square</td>
<td><span class="math inline">L^2</span></td>
<td><span class="math inline">\theta = (xx^T + \lambda I)^{-1}xy^T</span></td>
</tr>
<tr class="odd">
<td>Lasso regression</td>
<td>square</td>
<td><span class="math inline">L^1</span></td>
<td>no analytical, sub gradient descent</td>
</tr>
</tbody>
</table>
<h3 id="non-parametric-models">Non parametric models</h3>
<h4 id="finding-a-metric">Finding a metric</h4>
<p>kNN decision trees random forests Maximum-entropy Markov models</p>
<h2 id="neural-networks">Neural networks</h2>
<p>TO-DO</p>
<h2 id="generative-problems">Generative Problems</h2>
<p>TO-DO</p>
<h2 id="model-validation">Model validation</h2>
<p>TO-DO</p>
<h2 id="learning-algorithm-selection">Learning algorithm selection</h2>
<p>TO-DO</p></div>
            <div class="d-none d-xl-block col-xl-2 bd-toc">
               <ul class="section-nav">
                  <li class="toc-entry"><ul>
<li><a href="#data-representation">Data representation</a>
<ul>
<li><a href="#definitions">Definitions</a></li>
<li><a href="#pre-treatments">Pre-treatments</a></li>
<li><a href="#handling-missing-data">Handling missing data</a></li>
<li><a href="#dimensionality-reduction">Dimensionality reduction</a></li>
</ul></li>
<li><a href="#discriminative-problems">Discriminative problems</a>
<ul>
<li><a href="#loss-and-risk-functions">Loss and risk functions</a></li>
<li><a href="#parametric-models">Parametric models</a></li>
<li><a href="#non-parametric-models">Non parametric models</a></li>
</ul></li>
<li><a href="#neural-networks">Neural networks</a></li>
<li><a href="#generative-problems">Generative Problems</a></li>
<li><a href="#model-validation">Model validation</a></li>
<li><a href="#learning-algorithm-selection">Learning algorithm selection</a></li>
</ul></li>
               </ul>
            </div>
         </div>
               </div>
            <!-- Add comment hosting service here -->
            <!-- Footer -->
            <footer class="footer text-muted">
               <div align="center">
                  <!-- Update licences -->
                  Content is available under <a href="https://creativecommons.org/licenses/by-sa/3.0/" target="_blank" rel="noopener">CC BY-SA 3.0</a>
                  &nbsp;|&nbsp;
                  Sourcecode licensed under <a href="https://www.gnu.org/licenses/gpl-3.0.en.html" target="_blank" rel="noopener">GPL-3.0</a>
                  <br />
                  <!-- Please keep the following line -->
                  Built with <a href="https://www.pandoc.org" target="_blank" rel="noopener">Pandoc</a> 
                  using <a href="https://github.com/ashki23/pandoc-bootstrap" target="_blank" rel="noopener">pandoc-bootstrap</a> theme
                  <br />
                  <!-- Update copyright -->
                  Copyright, Author Name
               </div>
            </footer>
            <!-- Add global site tag (gtag.js) and site analytics here -->
            <!-- JS, Popper.js, and jQuery -->
      <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
      <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
      <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js" integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI" crossorigin="anonymous"></script>
      <!-- Mathjax -->
      <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      <script>
         /* Bootstrap styles to tables */
         function bootstrapStylePandocTables() {
         $('tr.header').parent('thead').parent('table').addClass('table table-condensed'); }
         $(document).ready(function () { bootstrapStylePandocTables(); });
         /* Adjust the height when click the toc */
         var shiftWindow = function() { scrollBy(0, -60) };
         window.addEventListener("hashchange", shiftWindow);
         function load() { if (window.location.hash) shiftWindow(); }
      </script>
   </body>
</html>
