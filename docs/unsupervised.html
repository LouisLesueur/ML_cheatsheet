<!doctype html>
<html>
   <head>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
      <meta name="generator" content="pandoc">
      <meta name="author" content="Louis Lesueur">
      
      <title>Unsupervised learning - synthesis</title>
      <!-- Bootstrap -->
      <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">
      <!-- Font-awesome -->
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
      <!-- Styles -->
      <link rel="stylesheet" href="https://ashki23.github.io/styles.css">
            <!-- Add favicon here -->
            <!-- Add site-verifications here -->
            <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js"></script>
              <script>document.addEventListener("DOMContentLoaded", function () {
               var mathElements = document.getElementsByClassName("math");
               var macros = [];
               for (var i = 0; i < mathElements.length; i++) {
                var texText = mathElements[i].firstChild;
                if (mathElements[i].tagName == "SPAN") {
                 katex.render(texText.data, mathElements[i], {
                  displayMode: mathElements[i].classList.contains('display'),
                  throwOnError: false,
                  macros: macros,
                  fleqn: false
                 });
              }}});
              </script>
              <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" />
              <link rel="stylesheet" href="custom.css" />
         </head>
   <body>
            <!-- From https://getbootstrap.com/docs/4.5/components/navbar/ -->
            <nav class="navbar fixed-top navbar-expand-lg navbar-dark bg-dark">
               <a class="navbar-brand" href="#">Machine Learning synthesis</a>
               <div class="collapse navbar-collapse" id="navbarSupportedContent">
                  <ul class="navbar-nav mr-auto">
                     <li class="nav-item active">
                        <a class="nav-link" href="index.html">Supervised <span class="sr-only">(current)</span></a>
                     </li>
                     <li class="nav-item">
                        <a class="nav-link" href="unsupervised.html">Unsupervised</a>
                     </li>
                     <li class="nav-item">
                        <a class="nav-link" href="reinforcement.html">Reinforcement</a>
                     </li>
                  </ul>
               </div>

            </nav>
            <div class="container">
         <h1 class="title">Unsupervised learning - synthesis</h1>
                  <div class="row">
            <div class="col-xl-10"><h2 id="definitions">Definitions</h2>
<p>unlike supervised learning, in unsupervised learning the input data are not labelized. The goal is to fond patterns in them. In other words, the dataset <span class="math inline">\mathcal{D}</span> is a radom sample <span class="math inline">\{X_1, \dots, X_n \}</span> from an unknown random variable <span class="math inline">X</span>, and we want to find patterns in them.</p>
<h2 id="dimension-reduction">Dimension reduction</h2>
<h3 id="principal-component-analysis">Principal component analysis</h3>
<h4 id="some-reminders">Some reminders</h4>
<ul>
<li>Correlation matrix of the dataset: <span class="math inline">K_n = \frac{1}{n} \sum_i (x_i- \bar{x_n})^T (x_i-\bar{x_n}) = \frac{1}{n} \sum_i x_i^Tx_i - \bar{x_n}^T \bar{x_n}</span></li>
<li>Empirical variance: <span class="math inline">\sigma_n^2 = \frac{1}{n} \sum_i ||x_i - \bar{x_n}||^2 = tr(K_n)</span></li>
<li>Empirical correlation between features: <span class="math inline">Corr(x^j, x^{j&#39;}) = \frac{K_n^{jj&#39;}}{\sqrt{K_n^{jj}} \sqrt{K_n^{j&#39;j&#39;}}} = \frac{K_n^{jj&#39;}}{\sigma_n^j \sigma_n^{j&#39;}}</span></li>
</ul>
<h4 id="pca-principle">PCA principle</h4>
<p>To apply PCA, we suppose that data is centered (ie <span class="math inline">\bar{x_n}=0</span>). If the features are too different, the dataset could benefit from standardization.</p>
<p>Suppose that they are <span class="math inline">p</span> features (ie a data vector is in <span class="math inline">\mathbb{R^p}</span>), the goal of PCA is to find an orthogonal projection on a subspace with dimension <span class="math inline">k&lt;p</span> that best preserve the original shape of the set, that ‘lose less information possible’.</p>
<p>As the data are centered, and using Pythagoras’s theorem, orthogonal projection on <span class="math inline">H</span> gives: <span class="math display">
\sigma_n^2 = (\sigma_n^H)^2 + \frac{1}{n} \sum_i ||x_i - x_i^H||^2
</span></p>
<p>We want to maximize <span class="math inline">(\sigma_n^H)^2</span>, to preserve the maximum of information. But, for <span class="math inline">H_k = Span(e_1, \dot, e_k)</span> we have: <span class="math display">
(\sigma_n^{H_k})^2 = \sum_{j=1}^k \lambda_j
</span> where the <span class="math inline">\lambda_j</span> are the eigenvalues of <span class="math inline">K_n</span>.</p>
<p>And, for all linear spaces <span class="math inline">H</span> with dimension <span class="math inline">k</span>, we have: <span class="math inline">(\sigma_n^H)^2 \leq (\sigma_n^{H_k})^2</span>. Hence, <strong>the computation of optimal subspaces is reduced to the diagonalisation of the empirical covariance matrix</strong>.</p>
<h4 id="interpretation-of-pca">Interpretation of PCA</h4>
<p>The <span class="math inline">l</span>-th principal component is the column vector <span class="math inline">c^l = \sum_{j=1}^p e_l^j x^j</span></p>
<p>The principal components are uncorrelated: <span class="math inline">Corr(c^l, c^{l&#39;}) = 0</span> if <span class="math inline">l \neq l&#39;</span>, furthermore: <span class="math inline">\forall l \in \{1, \dots, p \}, Corr(c^l, x^j) = \frac{\sqrt{\lambda_l}e_l^j}{\sigma_n^j}</span> and: <span class="math display">
\sum_{l=1}^p Corr(c^l, x^j)^2 = 1
</span></p>
<p>The last equation implies that the point of <span class="math inline">\mathbb{R}^p</span> with coordinates <span class="math inline">(Corr(c^1,x^j), \dots, Corr(c^p,x^j))</span> are on the unit sphere of <span class="math inline">\mathbb{R}^p</span>. It is called the correlation sphere. The plane spanned by <span class="math inline">(c^1, c^2)</span> is called the first factorial plane. When a feature is closed to the center of a factorial plan, it means that the selected principal component aren’t enough to explain its variance in the dataset.</p>
<h4 id="proportion-of-variance-explained-by-pca">Proportion of variance explained by PCA:</h4>
<p>The propotion of variance explained by PCA is the ratio:</p>
<p><span class="math display">
\frac{(\sigma_n^{H_k})^2}{\sigma_n^2} = \frac{\sum_{j=1}^k \lambda_j}{\sum_{j=1}^p \lambda_j}
</span></p>
<p>It is usefull to control how much information is lost by performing dimension reduction with PCA.</p>
<h3 id="independent-component-analysis">Independent component analysis</h3>
<h2 id="clustering">Clustering</h2>
<h3 id="k-means"><span class="math inline">k</span>-means</h3>
<p>In <span class="math inline">k</span>-means, we want to partition the data in <span class="math inline">k</span> subsets <span class="math inline">S = \{S_1,\dots,S_k\}</span> so as to minimize the within-cluster sum of squares (which is linked to variance). Formally, we want to find:</p>
<p><span class="math display">
\arg \min_S \sum_{i=1}^k \sum_{x \in S_i} ||x- \mu_i ||^2 = \arg \min_S \sum_{i=1}^k |S_i| \text{Var} S_i = \arg \min_S \sum_{i=1}^k \frac{1}{2|S_i|} \sum_{x,y \in S_i} ||x - y ||^2
</span></p>
<p>To solve the problem, the idea is to chose <span class="math inline">k</span> initial points for cluster centroids, calculate the Voronoi regions associated to them and update them by the formed cluster means.</p>
<h3 id="hierarchical-clustering">hierarchical clustering</h3>
<p>In hierarchical clstering, for a given metric and a linkage function (distance between clusters). The clustering consists in building a dendogram following the steps (bottom-up approach is presented here, but one can also imagine top-down methods):</p>
<ol type="1">
<li>Put each data in a unique cluster</li>
<li>Compute the pairwise linkage between each cluster</li>
<li>Group the two clusters with smallest linkage</li>
<li>Repeat steps two and three until there is on cluster</li>
</ol>
<p>Here are some common linkage functions:</p>
<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 75%" />
</colgroup>
<thead>
<tr class="header">
<th>Name</th>
<th>Formula (<span class="math inline">A</span> and <span class="math inline">B</span> are two sets of observation)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Maximum</td>
<td><span class="math inline">d(A,B) = \max(d(a,b) | a \in A, b \in B)</span></td>
</tr>
<tr class="even">
<td>Minimum</td>
<td><span class="math inline">d(A,B) = \min(d(a,b) | a \in A, b \in B)</span></td>
</tr>
<tr class="odd">
<td>UPGMA</td>
<td><span class="math inline">d(A,B) = \frac{1}{|A| |B|} \sum_{a \in A} \sum_{b \in B} d(a,b)</span></td>
</tr>
<tr class="even">
<td>WPGMA</td>
<td><span class="math inline">d(A \cup B, C) = \frac{d(A,C) + d(B,C)}{2}</span></td>
</tr>
<tr class="odd">
<td>UPGMC</td>
<td><span class="math inline">d(1,B) = ||\mu_A - \mu_B||</span> where <span class="math inline">\mu_A</span> and <span class="math inline">\mu_B</span> are the centroids of the clusters</td>
</tr>
<tr class="even">
<td>Energy distance</td>
<td><span class="math inline">d(A,B) = \frac{2}{nm} \sum_{i,j} ||a_i - b_j||_2 - \frac{1}{n^2} \sum_{i,j} ||a_i - a_j||_2 - \frac{1}{m^2} \sum_{i,j} ||b_i - a_j||_2</span></td>
</tr>
<tr class="odd">
<td>Ward distance</td>
<td><span class="math inline">d(A,B) = \frac{nm}{n+m}d(\mu_A, \mu_B)</span></td>
</tr>
</tbody>
</table>
<h3 id="density-based-methods-dbscan-and-optics">Density based methods: DBSCAN and OPTICS</h3>
<p>DBSCAN use the notion of neighbourhood to perform clustering. There are two parameters:</p>
<ul>
<li><span class="math inline">\epsilon</span>, the radius of the neighborhood</li>
<li><span class="math inline">m</span> the minimum number of points in an <span class="math inline">\epsilon</span>-neighborhood</li>
</ul>
<p>For <span class="math inline">p \in \mathcal{D}</span>, let consider <span class="math inline">N_\epsilon(p) = \{ q \in \mathcal{D} | d(p,q) &lt; \epsilon \}</span>. We say that:</p>
<ul>
<li><span class="math inline">q</span> is directly density-reachable from <span class="math inline">p</span> if <span class="math inline">q \in N_\epsilon(p)</span> and <span class="math inline">|N_\epsilon(p)| \geq m</span>.</li>
<li><span class="math inline">q</span> is density-reachable from <span class="math inline">p</span> if there exist a sequence of two by two directly density-reachable points between them.</li>
<li><span class="math inline">q</span> is density-connected to <span class="math inline">p</span> if there exist <span class="math inline">o \in \mathcal{D}</span> such as both <span class="math inline">p</span> and <span class="math inline">q</span> are density-reachable from <span class="math inline">o</span></li>
</ul>
<p>Then, clusters are build by grouping points according to density-connectivity. And for a given cluster, we can distinguish core points (with dense neighborhood) from border points (which are in a cluster but with not dense neighborhood), and from noise (which are neither core neither border).</p>
<p>OPTICS algorithm is another density-based clustering methods, using the same idea.</p>
<h3 id="expectationmaximization-em">Expectation–maximization (EM)</h3>
<h2 id="density-estimation">Density estimation</h2>
<p>See <a href="https://www.ssc.wisc.edu/~bhansen/718/NonParametrics1.pdf">here</a> for a detailed theory</p>
<h3 id="kernel-density-estimation">Kernel density estimation</h3>
<p>In kernel density estimation, we want to find the density function of the data-distribution.</p>
<p>Naturally, the distribution <span class="math inline">F</span> can be estimated by the EDF: <span class="math inline">\hat{F}(x) = n^{-1}\sum_i \mathbb{1}_{\{X_i \leq x}</span>.</p>
<p>To have an estimation of <span class="math inline">f</span> which is not a set of mass points, one can consider a discrete derivative of the form: <span class="math inline">\hat{f}(x) = \frac{\hat{F}(x+h)-\hat{F}(x-h)}{2h} = \frac{1}{nh} \sum_i k(\frac{X_i-x}{h})</span> with <span class="math inline">k(u) = \frac{1}{2} \mathbb{1}_{|u| \leq 1}</span>. It is a special case of kernel estimator, which have the general form:</p>
<p><span class="math display">
\hat{f}(x) =\frac{1}{nh} \sum_i k(\frac{X_i-x}{h})
</span></p>
<p>where <span class="math inline">k</span> is a kernel function (ie <span class="math inline">\int_\mathbb{R}k = 1</span>)</p>
<h4 id="some-kernel-functions-and-properties">Some kernel functions and properties</h4>
<ul>
<li>A non-negative kernel is such as: <span class="math inline">k \geq 0</span> on <span class="math inline">\mathbb{R}</span> (in this case <span class="math inline">k</span> is a probability density)</li>
<li>The moments of a kernel are: <span class="math inline">\kappa_j(k) = \int_\mathbb{R} u^jk(u)du</span></li>
<li>A symmetric kernel satisfies <span class="math inline">k(u)=k(-u)</span> for all <span class="math inline">u</span>. In this case, all odd moments are zero.</li>
<li>The order <span class="math inline">\nu</span> of a kernel is defined as the order of its first non-zero moment.</li>
</ul>
<p>Here are some second order kernels:</p>
<table>
<thead>
<tr class="header">
<th>Kernel</th>
<th>Equation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Uniform</td>
<td><span class="math inline">\frac{1}{2} \mathbb{1}_{\{|u| \leq 1 \}}</span></td>
</tr>
<tr class="even">
<td>Epanechnikov</td>
<td><span class="math inline">\frac{3}{4}(1-u^2) \mathbb{1}_{\{|u| \leq 1 \}}</span></td>
</tr>
<tr class="odd">
<td>Gaussian</td>
<td><span class="math inline">\frac{1}{\sqrt{2 \pi}} \exp{(-\frac{u^2}{2})}</span></td>
</tr>
</tbody>
</table>
<h2 id="anomaly-detection">Anomaly detection</h2>
<ul>
<li>Local Outlier Factor</li>
<li>Isolation Forest</li>
</ul>
<h2 id="neural-networks">Neural Networks</h2>
<ul>
<li>Autoencoders</li>
<li>Deep Belief Nets</li>
<li>Hebbian Learning</li>
<li>Generative adversarial networks</li>
<li>Self-organizing map</li>
</ul></div>
            <div class="d-none d-xl-block col-xl-2 bd-toc">
               <ul class="section-nav">
                  <li class="toc-entry"><ul>
<li><a href="#definitions">Definitions</a></li>
<li><a href="#dimension-reduction">Dimension reduction</a>
<ul>
<li><a href="#principal-component-analysis">Principal component analysis</a></li>
<li><a href="#independent-component-analysis">Independent component analysis</a></li>
</ul></li>
<li><a href="#clustering">Clustering</a>
<ul>
<li><a href="#k-means"><span class="math inline">k</span>-means</a></li>
<li><a href="#hierarchical-clustering">hierarchical clustering</a></li>
<li><a href="#density-based-methods-dbscan-and-optics">Density based methods: DBSCAN and OPTICS</a></li>
<li><a href="#expectationmaximization-em">Expectation–maximization (EM)</a></li>
</ul></li>
<li><a href="#density-estimation">Density estimation</a>
<ul>
<li><a href="#kernel-density-estimation">Kernel density estimation</a></li>
</ul></li>
<li><a href="#anomaly-detection">Anomaly detection</a></li>
<li><a href="#neural-networks">Neural Networks</a></li>
</ul></li>
               </ul>
            </div>
         </div>
               </div>
            <!-- Add comment hosting service here -->
            <!-- Footer -->
            <footer class="footer text-muted">
               <div align="center">
                  <!-- Update licences -->
                  Content is available under <a href="https://creativecommons.org/licenses/by-sa/3.0/" target="_blank" rel="noopener">CC BY-SA 3.0</a>
                  &nbsp;|&nbsp;
                  Sourcecode licensed under <a href="https://www.gnu.org/licenses/gpl-3.0.en.html" target="_blank" rel="noopener">GPL-3.0</a>
                  <br />
                  <!-- Please keep the following line -->
                  Built with <a href="https://www.pandoc.org" target="_blank" rel="noopener">Pandoc</a> 
                  using <a href="https://github.com/ashki23/pandoc-bootstrap" target="_blank" rel="noopener">pandoc-bootstrap</a> theme
                  <br />
                  <!-- Update copyright -->
                  Copyright, Author Name
               </div>
            </footer>
            <!-- Add global site tag (gtag.js) and site analytics here -->
            <!-- JS, Popper.js, and jQuery -->
      <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
      <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
      <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js" integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI" crossorigin="anonymous"></script>
      <!-- Mathjax -->
      <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      <script>
         /* Bootstrap styles to tables */
         function bootstrapStylePandocTables() {
         $('tr.header').parent('thead').parent('table').addClass('table table-condensed'); }
         $(document).ready(function () { bootstrapStylePandocTables(); });
         /* Adjust the height when click the toc */
         var shiftWindow = function() { scrollBy(0, -60) };
         window.addEventListener("hashchange", shiftWindow);
         function load() { if (window.location.hash) shiftWindow(); }
      </script>
   </body>
</html>
