<!doctype html>
<html>
   <head>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
      <meta name="generator" content="pandoc">
      <meta name="author" content="Louis Lesueur">
      
      <title>Unsupervised learning - synthesis</title>
      <!-- Bootstrap -->
      <link rel="stylesheet" href="css/bootstrap.min.css">
      <!-- Font-awesome -->
      <link rel="stylesheet" href="css/font-awesome.min.css">
      <!-- Styles -->
      <link rel="stylesheet" href="css/styles.css">
            <!-- Add favicon here -->
            <!-- Add site-verifications here -->
            <script src="js/katex.min.js"></script>
              <script>document.addEventListener("DOMContentLoaded", function () {
               var mathElements = document.getElementsByClassName("math");
               var macros = [];
               for (var i = 0; i < mathElements.length; i++) {
                var texText = mathElements[i].firstChild;
                if (mathElements[i].tagName == "SPAN") {
                 katex.render(texText.data, mathElements[i], {
                  displayMode: mathElements[i].classList.contains('display'),
                  throwOnError: false,
                  macros: macros,
                  fleqn: false
                 });
              }}});
              </script>
              <link rel="stylesheet" href="css/katex.min.css" />
         </head>
   <body>
            <!-- From https://getbootstrap.com/docs/4.5/components/navbar/ -->
            <nav class="navbar fixed-top navbar-expand-lg navbar-dark bg-dark">
               <a class="navbar-brand" href="#">Machine Learning synthesis</a>
               <div class="collapse navbar-collapse" id="navbarSupportedContent">
                  <ul class="navbar-nav mr-auto">
                     <li class="nav-item active">
                        <a class="nav-link" href="index.html">Supervised <span class="sr-only">(current)</span></a>
                     </li>
                     <li class="nav-item">
                        <a class="nav-link" href="unsupervised.html">Unsupervised</a>
                     </li>
                     <li class="nav-item">
                        <a class="nav-link" href="graphical.html">Graphical models</a>
                     </li>
                     <li class="nav-item">
                        <a class="nav-link" href="reinforcement.html">Reinforcement</a>
                     </li>
                  </ul>
               </div>

            </nav>
            <div class="container">
         <h1 class="title">Unsupervised learning - synthesis</h1>
                  <div class="row">
            <div class="col-xl-10"><h2 id="definitions">Definitions</h2>
<p>unlike supervised learning, in unsupervised learning the input data are not labelized. The goal is to fond patterns in them. In other words, the dataset <span class="math inline">\mathcal{D}</span> is a radom sample <span class="math inline">\{X_1, \dots, X_n \}</span> from an unknown random variable <span class="math inline">X</span>, and we want to find patterns in them.</p>
<h2 id="dimension-reduction">Dimension reduction</h2>
<h3 id="principal-component-analysis">Principal component analysis</h3>
<h4 id="some-reminders">Some reminders</h4>
<ul>
<li>Correlation matrix of the dataset: <span class="math inline">K_n = \frac{1}{n} \sum_i (x_i- \bar{x_n})^T (x_i-\bar{x_n}) = \frac{1}{n} \sum_i x_i^Tx_i - \bar{x_n}^T \bar{x_n}</span></li>
<li>Empirical variance: <span class="math inline">\sigma_n^2 = \frac{1}{n} \sum_i ||x_i - \bar{x_n}||^2 = tr(K_n)</span></li>
<li>Empirical correlation between features: <span class="math inline">Corr(x^j, x^{j&#39;}) = \frac{K_n^{jj&#39;}}{\sqrt{K_n^{jj}} \sqrt{K_n^{j&#39;j&#39;}}} = \frac{K_n^{jj&#39;}}{\sigma_n^j \sigma_n^{j&#39;}}</span></li>
</ul>
<h4 id="pca-principle">PCA principle</h4>
<p>To apply PCA, we suppose that data is centered (ie <span class="math inline">\bar{x_n}=0</span>). If the features are too different, the dataset could benefit from standardization.</p>
<p>Suppose that they are <span class="math inline">p</span> features (ie a data vector is in <span class="math inline">\mathbb{R^p}</span>), the goal of PCA is to find an orthogonal projection on a subspace with dimension <span class="math inline">k&lt;p</span> that best preserve the original shape of the set, that ‘lose less information possible’.</p>
<p>As the data are centered, and using Pythagoras’s theorem, orthogonal projection on <span class="math inline">H</span> gives: <span class="math display">
\sigma_n^2 = (\sigma_n^H)^2 + \frac{1}{n} \sum_i ||x_i - x_i^H||^2
</span></p>
<p>We want to maximize <span class="math inline">(\sigma_n^H)^2</span>, to preserve the maximum of information. But, for <span class="math inline">H_k = Span(e_1, \dot, e_k)</span> we have: <span class="math display">
(\sigma_n^{H_k})^2 = \sum_{j=1}^k \lambda_j
</span> where the <span class="math inline">\lambda_j</span> are the eigenvalues of <span class="math inline">K_n</span>.</p>
<p>And, for all linear spaces <span class="math inline">H</span> with dimension <span class="math inline">k</span>, we have: <span class="math inline">(\sigma_n^H)^2 \leq (\sigma_n^{H_k})^2</span>. Hence, <strong>the computation of optimal subspaces is reduced to the diagonalisation of the empirical covariance matrix</strong>.</p>
<h4 id="interpretation-of-pca">Interpretation of PCA</h4>
<p>The <span class="math inline">l</span>-th principal component is the column vector <span class="math inline">c^l = \sum_{j=1}^p e_l^j x^j</span></p>
<p>The principal components are uncorrelated: <span class="math inline">Corr(c^l, c^{l&#39;}) = 0</span> if <span class="math inline">l \neq l&#39;</span>, furthermore: <span class="math inline">\forall l \in \{1, \dots, p \}, Corr(c^l, x^j) = \frac{\sqrt{\lambda_l}e_l^j}{\sigma_n^j}</span> and: <span class="math display">
\sum_{l=1}^p Corr(c^l, x^j)^2 = 1
</span></p>
<p>The last equation implies that the point of <span class="math inline">\mathbb{R}^p</span> with coordinates <span class="math inline">(Corr(c^1,x^j), \dots, Corr(c^p,x^j))</span> are on the unit sphere of <span class="math inline">\mathbb{R}^p</span>. It is called the correlation sphere. The plane spanned by <span class="math inline">(c^1, c^2)</span> is called the first factorial plane. When a feature is closed to the center of a factorial plan, it means that the selected principal component aren’t enough to explain its variance in the dataset.</p>
<h4 id="proportion-of-variance-explained-by-pca">Proportion of variance explained by PCA:</h4>
<p>The propotion of variance explained by PCA is the ratio:</p>
<p><span class="math display">
\frac{(\sigma_n^{H_k})^2}{\sigma_n^2} = \frac{\sum_{j=1}^k \lambda_j}{\sum_{j=1}^p \lambda_j}
</span></p>
<p>It is usefull to control how much information is lost by performing dimension reduction with PCA.</p>
<h3 id="independent-component-analysis">Independent component analysis</h3>
<h2 id="clustering">Clustering</h2>
<h3 id="k-means"><span class="math inline">k</span>-means</h3>
<p>In <span class="math inline">k</span>-means, we want to partition the data in <span class="math inline">k</span> subsets <span class="math inline">S = \{S_1,\dots,S_k\}</span> so as to minimize the within-cluster sum of squares (which is linked to variance). Formally, we want to find:</p>
<p><span class="math display">
\arg \min_S \sum_{i=1}^k \sum_{x \in S_i} ||x- \mu_i ||^2 = \arg \min_S \sum_{i=1}^k |S_i| \text{Var} S_i = \arg \min_S \sum_{i=1}^k \frac{1}{2|S_i|} \sum_{x,y \in S_i} ||x - y ||^2
</span></p>
<p>To solve the problem, the idea is to chose <span class="math inline">k</span> initial points for cluster centroids, calculate the Voronoi regions associated to them and update them by the formed cluster means.</p>
<h3 id="hierarchical-clustering">hierarchical clustering</h3>
<p>In hierarchical clstering, for a given metric and a linkage function (distance between clusters). The clustering consists in building a dendogram following the steps (bottom-up approach is presented here, but one can also imagine top-down methods):</p>
<ol type="1">
<li>Put each data in a unique cluster</li>
<li>Compute the pairwise linkage between each cluster</li>
<li>Group the two clusters with smallest linkage</li>
<li>Repeat steps two and three until there is on cluster</li>
</ol>
<p>Here are some common linkage functions:</p>
<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 75%" />
</colgroup>
<thead>
<tr class="header">
<th>Name</th>
<th>Formula (<span class="math inline">A</span> and <span class="math inline">B</span> are two sets of observation)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Maximum</td>
<td><span class="math inline">d(A,B) = \max(d(a,b) | a \in A, b \in B)</span></td>
</tr>
<tr class="even">
<td>Minimum</td>
<td><span class="math inline">d(A,B) = \min(d(a,b) | a \in A, b \in B)</span></td>
</tr>
<tr class="odd">
<td>UPGMA</td>
<td><span class="math inline">d(A,B) = \frac{1}{|A| |B|} \sum_{a \in A} \sum_{b \in B} d(a,b)</span></td>
</tr>
<tr class="even">
<td>WPGMA</td>
<td><span class="math inline">d(A \cup B, C) = \frac{d(A,C) + d(B,C)}{2}</span></td>
</tr>
<tr class="odd">
<td>UPGMC</td>
<td><span class="math inline">d(1,B) = ||\mu_A - \mu_B||</span> where <span class="math inline">\mu_A</span> and <span class="math inline">\mu_B</span> are the centroids of the clusters</td>
</tr>
<tr class="even">
<td>Energy distance</td>
<td><span class="math inline">d(A,B) = \frac{2}{nm} \sum_{i,j} ||a_i - b_j||_2 - \frac{1}{n^2} \sum_{i,j} ||a_i - a_j||_2 - \frac{1}{m^2} \sum_{i,j} ||b_i - a_j||_2</span></td>
</tr>
<tr class="odd">
<td>Ward distance</td>
<td><span class="math inline">d(A,B) = \frac{nm}{n+m}d(\mu_A, \mu_B)</span></td>
</tr>
</tbody>
</table>
<h3 id="density-based-methods-dbscan-and-optics">Density based methods: DBSCAN and OPTICS</h3>
<p>DBSCAN use the notion of neighbourhood to perform clustering. There are two parameters:</p>
<ul>
<li><span class="math inline">\epsilon</span>, the radius of the neighborhood</li>
<li><span class="math inline">m</span> the minimum number of points in an <span class="math inline">\epsilon</span>-neighborhood</li>
</ul>
<p>For <span class="math inline">p \in \mathcal{D}</span>, let consider <span class="math inline">N_\epsilon(p) = \{ q \in \mathcal{D} | d(p,q) &lt; \epsilon \}</span>. We say that:</p>
<ul>
<li><span class="math inline">q</span> is directly density-reachable from <span class="math inline">p</span> if <span class="math inline">q \in N_\epsilon(p)</span> and <span class="math inline">|N_\epsilon(p)| \geq m</span>.</li>
<li><span class="math inline">q</span> is density-reachable from <span class="math inline">p</span> if there exist a sequence of two by two directly density-reachable points between them.</li>
<li><span class="math inline">q</span> is density-connected to <span class="math inline">p</span> if there exist <span class="math inline">o \in \mathcal{D}</span> such as both <span class="math inline">p</span> and <span class="math inline">q</span> are density-reachable from <span class="math inline">o</span></li>
</ul>
<p>Then, clusters are build by grouping points according to density-connectivity. And for a given cluster, we can distinguish core points (with dense neighborhood) from border points (which are in a cluster but with not dense neighborhood), and from noise (which are neither core neither border).</p>
<p>OPTICS algorithm is another density-based clustering methods, using the same idea.</p>
<h3 id="expectationmaximization-em">Expectation–maximization (EM)</h3>
<h2 id="density-estimation">Density estimation</h2>
<p>See <a href="https://www.ssc.wisc.edu/~bhansen/718/NonParametrics1.pdf">here</a> for a detailed theory</p>
<h3 id="kernel-density-estimation">Kernel density estimation</h3>
<p>In kernel density estimation, we want to find the density function of the data-distribution.</p>
<p>Naturally, the distribution <span class="math inline">F</span> can be estimated by the EDF: <span class="math inline">\hat{F}(x) = n^{-1}\sum_i \mathbb{1}_{\{X_i \leq x}</span>.</p>
<p>To have an estimation of <span class="math inline">f</span> which is not a set of mass points, one can consider a discrete derivative of the form: <span class="math inline">\hat{f}(x) = \frac{\hat{F}(x+h)-\hat{F}(x-h)}{2h} = \frac{1}{nh} \sum_i k(\frac{X_i-x}{h})</span> with <span class="math inline">k(u) = \frac{1}{2} \mathbb{1}_{|u| \leq 1}</span>. It is a special case of kernel estimator, which have the general form:</p>
<p><span class="math display">
\hat{f}(x) =\frac{1}{nh} \sum_i k(\frac{X_i-x}{h})
</span></p>
<p>where <span class="math inline">k</span> is a kernel function (ie <span class="math inline">\int_\mathbb{R}k = 1</span>)</p>
<h4 id="some-kernel-functions-and-properties">Some kernel functions and properties</h4>
<ul>
<li>A non-negative kernel is such as: <span class="math inline">k \geq 0</span> on <span class="math inline">\mathbb{R}</span> (in this case <span class="math inline">k</span> is a probability density)</li>
<li>The moments of a kernel are: <span class="math inline">\kappa_j(k) = \int_\mathbb{R} u^jk(u)du</span></li>
<li>A symmetric kernel satisfies <span class="math inline">k(u)=k(-u)</span> for all <span class="math inline">u</span>. In this case, all odd moments are zero.</li>
<li>The order <span class="math inline">\nu</span> of a kernel is defined as the order of its first non-zero moment.</li>
</ul>
<p>Here are some second order kernels:</p>
<table>
<thead>
<tr class="header">
<th>Kernel</th>
<th>Equation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Uniform</td>
<td><span class="math inline">\frac{1}{2} \mathbb{1}_{\{|u| \leq 1 \}}</span></td>
</tr>
<tr class="even">
<td>Epanechnikov</td>
<td><span class="math inline">\frac{3}{4}(1-u^2) \mathbb{1}_{\{|u| \leq 1 \}}</span></td>
</tr>
<tr class="odd">
<td>Gaussian</td>
<td><span class="math inline">\frac{1}{\sqrt{2 \pi}} \exp{(-\frac{u^2}{2})}</span></td>
</tr>
</tbody>
</table>
<h2 id="anomaly-detection">Anomaly detection</h2>
<ul>
<li>Local Outlier Factor</li>
<li>Isolation Forest</li>
</ul>
<h2 id="neural-networks">Neural Networks</h2>
<h3 id="autoencoders">Autoencoders</h3>
<p>An autoencoder is a NN that is tarined to attempts to copy its input to its output. It is composed of two parts:</p>
<ul>
<li>The encoding function: <span class="math inline">f: \mathcal{X} \rightarrow \mathcal{F}</span></li>
<li>The decoding function: <span class="math inline">g: \mathcal{F} \rightarrow \mathcal{X}</span></li>
</ul>
<p>where <span class="math inline">\mathcal{F}</span> is the code space.</p>
<p>In fact, an autoencoder is learning the conditionnal distribution <span class="math inline">p_{AE}(h|x)</span> where <span class="math inline">h \in \mathcal{F}</span>. And we have: <span class="math inline">p_{encoder}(h|x) = p_{AE}(h|x)</span> and <span class="math inline">p_{decoder}(x|h) = p_{AE}(x|h)</span>. And the loss can be seen as a maximum-likelihood maximization, just like in supervized methods.</p>
<h4 id="vanilla-autoencoders">Vanilla autoencoders</h4>
<ul>
<li>Undercomplete autoencoder: code space dimension less than the input space.</li>
<li>Overcomplete autoencoder : code space dimension greater than the input space.</li>
</ul>
<p>The learning consists in minimizing a loss function: <span class="math display">
L(x,g(f(x))) = ||x-g(f(x))||^2
</span></p>
<p>If <span class="math inline">g</span> and <span class="math inline">f</span> are linear, the undercomplete autoencoder is simply learning PCA subspaces ! So non-linear autoencoder can be seen as a non linear generalization of PCA.</p>
<h4 id="regularized-autoencoders">Regularized autoencoders</h4>
<p>If the encoder and the decoder have too much capacity, it is possible that they don’t learn anything on the data distribution, but only specific things on the dataset (it is a kind of overfitting). For example, one can imagine an autoencoder which maps <span class="math inline">x_i</span> to <span class="math inline">i</span> and <span class="math inline">i</span> to <span class="math inline">x_i</span>. The learned subset of indexes tells nothing about the data distribution. That’s where regularization join the game.</p>
<h5 id="sparse-autoencoders-sae">Sparse Autoencoders (SAE)</h5>
<p>A sparse autoencoder involves a saprsity penalty <span class="math inline">\Omega(h)</span> on the code layer <span class="math inline">f(x)=h</span>: <span class="math display">
L(x,g(f(x))) + \Omega(h)
</span></p>
<p>They are typically used to learn features for other tasks such as classification.</p>
<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 37%" />
<col style="width: 37%" />
</colgroup>
<thead>
<tr class="header">
<th>Regularization therm</th>
<th><span class="math inline">\Omega(h)</span></th>
<th>Remarks</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>KL</td>
<td><span class="math inline">\sum_j KL(\rho || \frac{1}{n} \sum_i(h_j(x_i)))</span></td>
<td><span class="math inline">\rho</span> is the sparcity parameter, close to zero. This regularization penalizes average activation of the neurones from <span class="math inline">h</span> on the dataset for deviating from <span class="math inline">\rho</span>, and so force them to be inactive most of the time.</td>
</tr>
<tr class="even">
<td><span class="math inline">L^1</span> and <span class="math inline">L^2</span></td>
<td><span class="math inline">\lambda ||h||</span></td>
<td>It is known that these regularizations acheive sparcity.</td>
</tr>
</tbody>
</table>
<h5 id="contractive-autoencoders-cae">Contractive Autoencoders (CAE)</h5>
<p>Another regularization strategy consists in penalizing the gradient: <span class="math display">
L(x,g(f(x))) + \lambda \sum_i || \nabla_x h_i ||^2
</span></p>
<p>This forces the model to learn a function that does not changes much when <span class="math inline">x</span> changes slightly.</p>
<h4 id="denoising-autoencoders-dae">Denoising Autoencoders (DAE)</h4>
<p>A denoising autoencoder minimizes: <span class="math display">
L(x,g(f(\tilde{x})))
</span></p>
<p>where <span class="math inline">\tilde{x}</span> is a copy of <span class="math inline">x</span> that has been corrupted by some noise.</p>
<h3 id="variational-autoencoders-vae">Variational Autoencoders (VAE)</h3>
<p>VAE are a generative model. So the goal is to find the distibution <span class="math inline">p_\theta(x)</span> of the data. To do so, we suppose that the data are built from a latent space (containing the information), unknown and unobserved: <span class="math display">
p_\theta(x,z) = p_\theta(x|z)p_\theta(z)
</span> The corresponding margin-likelihood that we want to maximize is: <span class="math display">
p_\theta(x) = \int_\mathcal{Z} p_\theta(x|z)p_\theta(z)dz
</span></p>
<p>As the latent space is unknown, the terms of the integral are approximated by known distributions. But, this integral is most of the time untractable. And there is too much data to use classical estimation technics (Monte-Carlo) to compute it.</p>
<p>Knowing that, the goals of VAE are:</p>
<ul>
<li>Approximaet <span class="math inline">\theta</span> by ML, to mimic the latent process and generate new artificial data.</li>
<li>Approximate <span class="math inline">p_\theta(z|x)</span> to code data</li>
<li>Approximate the marginal inference of <span class="math inline">x</span> (to perform the same tasks as an autoencoder)</li>
</ul>
<p>Because of intractabilities, and because of the large dataset, we introduce <span class="math display">
q_\phi(z|x)
</span> Which will approximate <span class="math inline">p_\theta(z|x) = p_\theta(x|z)p_\theta(z)/p_\theta(x)</span></p>
<p>Here the autoencoder structure appears:</p>
<ul>
<li>The latent space can be seen as the autoencoder ‘code’ space</li>
<li><span class="math inline">q_\phi(z|x)</span> can be interpreted as a probabilistic encoder</li>
<li><span class="math inline">p_\theta(x|z)</span> can be interpreted as a probabilistic decoder</li>
</ul>
<p>Lets write the log-marginlikelihood as:</p>
<p><span class="math display">
\log(p_\theta(x_i)) = \mathbb{E}_{q_\phi(z|x)}(\log(p_\theta(x_i))) = KL(q_\phi(z|x_i)||p_\theta(z|x_i)) + \mathcal{L}(\theta,\phi,x_i)
</span></p>
<p>As the KL divergence is non-negative, the second term is called the (variational) lower-bound (by Jensen inequality) of the marginal likelihood. By Jensen inequality we have: <span class="math display">
\mathcal{L}(\theta,\phi,x_i) = \mathbb{E}_{q_\phi(z|x_i)}(-\log(q_\phi(z|x_i)) + \log(p_\theta(x_i,z))) = -KL(q_\phi(z|x_i) || p_\theta(z)) + \mathbb{E}_{q_\phi(z|x_i)}(\log(p_\theta(x_i|z)))
</span></p>
<p>To have an estimator compatible with backpropagation (else, estimating <span class="math inline">z</span> only by sampling on <span class="math inline">q_\phi(z|x_i)</span> would prevent backpropagation), we need to introduce a reparametrization of <span class="math inline">z</span> using a differentiable transformation <span class="math inline">g_\phi(\epsilon,x)</span> of an auxiliary noise variable <span class="math inline">\epsilon</span>. An estimation od <span class="math inline">z</span> will then be done by taking <span class="math inline">L</span> samples from the corresponding distibution: <span class="math display">
z_{i,l} = g_\phi(\epsilon_{i,l}, x_i) \text{   and   } \epsilon_l \sim p(\epsilon)
</span></p>
<p>As the <span class="math inline">KL</span> divergence is often directly integrable, a good estimator for <span class="math inline">\mathcal{L}</span> is:</p>
<p><span class="math display">
\hat{\mathcal{L}}(\theta, \phi, x_i) = -KL(q_\phi(z|x_i)||p_\theta(z)) + \frac{1}{L} \sum_{l=1}^L \log p_\theta(x_i | z_{i,l})
</span></p>
<p>And, of course: <span class="math inline">\hat{\mathcal{L}} = \sum_i \hat{\mathcal{L}}(\theta, \phi, x_i)</span></p>
<p>In VAE, we classicaly suppose that:</p>
<ul>
<li><span class="math inline">p_\theta(z) = \mathcal{N}(0,I)</span></li>
<li><span class="math inline">q_\phi(z|x_i) = \mathcal{N}(\mu_\phi(x_i), \sigma_\phi^2(x_i) I)</span> (multivariate gaussian, with size <span class="math inline">J</span>, where <span class="math inline">\mu_\phi</span> and <span class="math inline">\sigma_\phi</span> are the outputs of a gaussian MLP (<span class="math inline">\mu(x) = g(h(x)))</span> and <span class="math inline">\sigma(x) = f(h(x))</span> where <span class="math inline">f,g,h</span> are MLP parametrized by <span class="math inline">\theta</span>)</li>
<li><span class="math inline">z_{i,l} = \mu(x_i) + \sigma(x_i) \circ \epsilon_l</span> where <span class="math inline">\epsilon_l \sim \mathcal{N}(0,I)</span></li>
</ul>
<p>Then: <span class="math display">
\hat{\mathcal{L}}(\theta, \phi, x_i) = \frac{1}{2} \sum_j (1 + \log((\sigma_j(x_i))^2) - (\mu_j(x_i))^2 - (\sigma_j(x_i))^2 ) + \frac{1}{L} \sum_{l=1}^L \log p_\theta(x_i | z_{i,l})
</span></p>
<p>And <span class="math inline">p_\theta(x_i | z_{i,l})</span> is gaussian or Bernoulli MLP, depending on the type of data modelling.</p>
<ul>
<li><span class="math inline">\log p_\theta(x_i | z_{i,l}) = - \frac{1}{2 \sigma_\theta(z_{i,l})} || x_i - \mu_\theta(z_{i,l}) ||^2</span> if gaussian</li>
<li><span class="math inline">\log p_\theta(x_i | z_{i,l}) = \sum_i x_i \log(f_\theta(z_{i,l})) + (1-x_i) \log(1-f_\theta(z_{i,l}))</span> if Bernoulli (<span class="math inline">f_\theta</span> is a neural network)</li>
</ul>
<h4 id="beta-vae"><span class="math inline">\beta</span> VAE</h4>
<p>In practice the latent-space is often bigger than it could be, to limit this phenomenon one can add a disentangling parameter <span class="math inline">\beta</span>:</p>
<p><span class="math display">
\hat{\mathcal{L}}(\theta, \phi, x_i) = - \beta KL(q_\phi(z|x_i)||p_\theta(z)) + \frac{1}{L} \sum_{l=1}^L \log p_\theta(x_i | z_{i,l})
</span></p>
<p>if <span class="math inline">\beta=0</span> we fall back on classical likelihood maximization, if <span class="math inline">\beta=1</span> the bayesian view of the problem appears. So the parameter <span class="math inline">\beta</span> represent the learning pressure (it constraint the latent space to be small).</p>
<h4 id="generative-adversarial-networks-gan">Generative adversarial networks (GAN)</h4>
<h3 id="self-organizing-map">Self-organizing map</h3>
<h3 id="deep-belief-nets">Deep Belief Nets</h3>
<h3 id="hebbian-learning">Hebbian Learning</h3></div>
            <div class="d-none d-xl-block col-xl-2 bd-toc">
               <ul class="section-nav">
                  <li class="toc-entry"><ul>
<li><a href="#definitions">Definitions</a></li>
<li><a href="#dimension-reduction">Dimension reduction</a>
<ul>
<li><a href="#principal-component-analysis">Principal component analysis</a></li>
<li><a href="#independent-component-analysis">Independent component analysis</a></li>
</ul></li>
<li><a href="#clustering">Clustering</a>
<ul>
<li><a href="#k-means"><span class="math inline">k</span>-means</a></li>
<li><a href="#hierarchical-clustering">hierarchical clustering</a></li>
<li><a href="#density-based-methods-dbscan-and-optics">Density based methods: DBSCAN and OPTICS</a></li>
<li><a href="#expectationmaximization-em">Expectation–maximization (EM)</a></li>
</ul></li>
<li><a href="#density-estimation">Density estimation</a>
<ul>
<li><a href="#kernel-density-estimation">Kernel density estimation</a></li>
</ul></li>
<li><a href="#anomaly-detection">Anomaly detection</a></li>
<li><a href="#neural-networks">Neural Networks</a>
<ul>
<li><a href="#autoencoders">Autoencoders</a></li>
<li><a href="#variational-autoencoders-vae">Variational Autoencoders (VAE)</a></li>
<li><a href="#self-organizing-map">Self-organizing map</a></li>
<li><a href="#deep-belief-nets">Deep Belief Nets</a></li>
<li><a href="#hebbian-learning">Hebbian Learning</a></li>
</ul></li>
</ul></li>
               </ul>
            </div>
         </div>
               </div>
            <!-- Add comment hosting service here -->
            <!-- Footer -->
            <footer class="footer text-muted">
               <div align="center">
                  <!-- Update licences -->
                  Content is available under <a href="https://creativecommons.org/licenses/by-sa/3.0/" target="_blank" rel="noopener">CC BY-SA 3.0</a>
                  &nbsp;|&nbsp;
                  Sourcecode licensed under <a href="https://www.gnu.org/licenses/gpl-3.0.en.html" target="_blank" rel="noopener">GPL-3.0</a>
                  <br />
                  <!-- Please keep the following line -->
                  Built with <a href="https://www.pandoc.org" target="_blank" rel="noopener">Pandoc</a>
                  using <a href="https://github.com/ashki23/pandoc-bootstrap" target="_blank" rel="noopener">pandoc-bootstrap</a> theme
                  <br />
                  <!-- Update copyright -->
                  Copyright, Louis Lesueur
               </div>
            </footer>
            <!-- Add global site tag (gtag.js) and site analytics here -->
            <!-- JS, Popper.js, and jQuery -->
      <script src="js/jquery-3.5.1.slim.min.js" ></script>
      <script src="js/popper.min.js" ></script>
      <script src="js/bootstrap.min.js" ></script>
      <script>
         /* Bootstrap styles to tables */
         function bootstrapStylePandocTables() {
         $('tr.header').parent('thead').parent('table').addClass('table table-condensed'); }
         $(document).ready(function () { bootstrapStylePandocTables(); });
         /* Adjust the height when click the toc */
         var shiftWindow = function() { scrollBy(0, -60) };
         window.addEventListener("hashchange", shiftWindow);
         function load() { if (window.location.hash) shiftWindow(); }
      </script>
   </body>
</html>
